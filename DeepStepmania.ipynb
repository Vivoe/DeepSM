{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep StepMania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T07:33:53.593885Z",
     "start_time": "2019-02-19T07:33:53.067259Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from deepSM import SMDataset\n",
    "from deepSM import StepPlacement\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.utils.data as datautils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T07:33:53.688484Z",
     "start_time": "2019-02-19T07:33:53.685569Z"
    }
   },
   "outputs": [],
   "source": [
    "song_names = os.listdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T07:50:38.247142Z",
     "start_time": "2019-02-19T07:49:18.035150Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (1103) is greater than FFT size (1024), frame will be truncated. Increase NFFT to avoid.\n"
     ]
    }
   ],
   "source": [
    "reload(SMDataset)\n",
    "# dataset = SMDataset.SMDataset(song_names[10])\n",
    "dataset = SMDataset.get_dataset(song_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T07:50:38.252245Z",
     "start_time": "2019-02-19T07:50:38.248665Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "897448"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T07:50:38.311475Z",
     "start_time": "2019-02-19T07:50:38.258764Z"
    }
   },
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.75)\n",
    "test_size = len(dataset) - train_size\n",
    "train_ds, test_ds = datautils.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T07:50:38.317318Z",
     "start_time": "2019-02-19T07:50:38.312695Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(SMDataset)\n",
    "\n",
    "# sampler = SMDataset.WeightedRandomSampler(train_ds)\n",
    "\n",
    "train_loader = datautils.DataLoader(\n",
    "    train_ds, num_workers=4, \n",
    "    batch_size= 64,\n",
    "    shuffle=True)\n",
    "#     sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T08:00:34.089905Z",
     "start_time": "2019-02-19T07:50:38.318439Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] loss: 113.580\n",
      "[1,  1000] loss: 110.234\n",
      "[1,  1500] loss: 105.936\n",
      "[1,  2000] loss: 107.217\n",
      "[1,  2500] loss: 110.330\n",
      "[1,  3000] loss: 107.007\n",
      "[1,  3500] loss: 103.371\n",
      "[1,  4000] loss: 108.985\n",
      "[1,  4500] loss: 104.927\n",
      "[1,  5000] loss: 106.260\n",
      "[1,  5500] loss: 104.628\n",
      "[1,  6000] loss: 110.151\n",
      "[1,  6500] loss: 103.773\n",
      "[1,  7000] loss: 104.885\n",
      "[1,  7500] loss: 106.279\n",
      "[1,  8000] loss: 102.446\n",
      "[1,  8500] loss: 104.504\n",
      "[1,  9000] loss: 101.795\n",
      "[1,  9500] loss: 104.963\n",
      "[1, 10000] loss: 104.317\n",
      "[1, 10500] loss: 104.199\n",
      "Starting epoch 2\n",
      "[2,   500] loss: 104.080\n",
      "[2,  1000] loss: 105.344\n",
      "[2,  1500] loss: 105.517\n",
      "[2,  2000] loss: 101.107\n",
      "[2,  2500] loss: 102.722\n",
      "[2,  3000] loss: 101.244\n",
      "[2,  3500] loss: 102.519\n",
      "[2,  4000] loss: 102.873\n",
      "[2,  4500] loss: 102.590\n",
      "[2,  5000] loss: 98.182\n",
      "[2,  5500] loss: 99.362\n",
      "[2,  6000] loss: 101.354\n",
      "[2,  6500] loss: 97.968\n",
      "[2,  7000] loss: 100.563\n",
      "[2,  7500] loss: 99.420\n",
      "[2,  8000] loss: 99.674\n",
      "[2,  8500] loss: 98.871\n",
      "[2,  9000] loss: 99.360\n",
      "[2,  9500] loss: 96.051\n",
      "[2, 10000] loss: 99.750\n",
      "[2, 10500] loss: 99.601\n",
      "Starting epoch 3\n",
      "[3,   500] loss: 95.985\n",
      "[3,  1000] loss: 98.686\n",
      "[3,  1500] loss: 96.539\n",
      "[3,  2000] loss: 96.416\n",
      "[3,  2500] loss: 98.189\n",
      "[3,  3000] loss: 99.865\n",
      "[3,  3500] loss: 97.428\n",
      "[3,  4000] loss: 96.822\n",
      "[3,  4500] loss: 95.269\n",
      "[3,  5000] loss: 93.665\n",
      "[3,  5500] loss: 97.112\n",
      "[3,  6000] loss: 94.927\n",
      "[3,  6500] loss: 94.237\n",
      "[3,  7000] loss: 96.764\n",
      "[3,  7500] loss: 94.892\n",
      "[3,  8000] loss: 97.114\n",
      "[3,  8500] loss: 93.896\n",
      "[3,  9000] loss: 94.873\n",
      "[3,  9500] loss: 93.125\n",
      "[3, 10000] loss: 92.895\n",
      "[3, 10500] loss: 93.000\n",
      "Starting epoch 4\n",
      "[4,   500] loss: 93.283\n",
      "[4,  1000] loss: 93.019\n",
      "[4,  1500] loss: 91.222\n",
      "[4,  2000] loss: 92.343\n",
      "[4,  2500] loss: 91.490\n",
      "[4,  3000] loss: 91.714\n",
      "[4,  3500] loss: 88.903\n",
      "[4,  4000] loss: 90.087\n",
      "[4,  4500] loss: 89.992\n",
      "[4,  5000] loss: 91.472\n",
      "[4,  5500] loss: 89.386\n",
      "[4,  6000] loss: 87.335\n",
      "[4,  6500] loss: 93.075\n",
      "[4,  7000] loss: 91.108\n",
      "[4,  7500] loss: 91.119\n",
      "[4,  8000] loss: 91.539\n",
      "[4,  8500] loss: 89.506\n",
      "[4,  9000] loss: 86.766\n",
      "[4,  9500] loss: 89.449\n",
      "[4, 10000] loss: 85.447\n",
      "[4, 10500] loss: 89.986\n",
      "Starting epoch 5\n",
      "[5,   500] loss: 84.714\n",
      "[5,  1000] loss: 83.092\n",
      "[5,  1500] loss: 85.918\n",
      "[5,  2000] loss: 86.534\n",
      "[5,  2500] loss: 85.999\n",
      "[5,  3000] loss: 84.712\n",
      "[5,  3500] loss: 85.206\n",
      "[5,  4000] loss: 84.524\n",
      "[5,  4500] loss: 84.487\n",
      "[5,  5000] loss: 87.854\n",
      "[5,  5500] loss: 86.188\n",
      "[5,  6000] loss: 88.450\n",
      "[5,  6500] loss: 82.715\n",
      "[5,  7000] loss: 86.014\n",
      "[5,  7500] loss: 84.096\n",
      "[5,  8000] loss: 83.122\n",
      "[5,  8500] loss: 84.382\n",
      "[5,  9000] loss: 84.184\n",
      "[5,  9500] loss: 85.630\n",
      "[5, 10000] loss: 85.352\n",
      "[5, 10500] loss: 84.433\n",
      "Starting epoch 6\n",
      "[6,   500] loss: 81.087\n",
      "[6,  1000] loss: 79.146\n",
      "[6,  1500] loss: 81.446\n",
      "[6,  2000] loss: 79.544\n",
      "[6,  2500] loss: 85.405\n",
      "[6,  3000] loss: 80.440\n",
      "[6,  3500] loss: 81.761\n",
      "[6,  4000] loss: 82.542\n",
      "[6,  4500] loss: 78.200\n",
      "[6,  5000] loss: 85.254\n",
      "[6,  5500] loss: 78.096\n",
      "[6,  6000] loss: 76.854\n",
      "[6,  6500] loss: 82.392\n",
      "[6,  7000] loss: 80.007\n",
      "[6,  7500] loss: 79.575\n",
      "[6,  8000] loss: 78.656\n",
      "[6,  8500] loss: 79.000\n",
      "[6,  9000] loss: 81.630\n",
      "[6,  9500] loss: 78.445\n",
      "[6, 10000] loss: 80.227\n",
      "[6, 10500] loss: 79.278\n",
      "Starting epoch 7\n",
      "[7,   500] loss: 73.994\n",
      "[7,  1000] loss: 74.782\n",
      "[7,  1500] loss: 74.413\n",
      "[7,  2000] loss: 74.713\n",
      "[7,  2500] loss: 78.263\n",
      "[7,  3000] loss: 77.660\n",
      "[7,  3500] loss: 73.420\n",
      "[7,  4000] loss: 75.214\n",
      "[7,  4500] loss: 76.128\n",
      "[7,  5000] loss: 78.712\n",
      "[7,  5500] loss: 73.625\n",
      "[7,  6000] loss: 76.446\n",
      "[7,  6500] loss: 75.636\n",
      "[7,  7000] loss: 76.507\n",
      "[7,  7500] loss: 75.963\n",
      "[7,  8000] loss: 75.942\n",
      "[7,  8500] loss: 75.986\n",
      "[7,  9000] loss: 76.286\n",
      "[7,  9500] loss: 77.629\n",
      "[7, 10000] loss: 78.004\n",
      "[7, 10500] loss: 77.153\n",
      "Starting epoch 8\n",
      "[8,   500] loss: 71.682\n",
      "[8,  1000] loss: 71.449\n",
      "[8,  1500] loss: 69.977\n",
      "[8,  2000] loss: 72.217\n",
      "[8,  2500] loss: 71.498\n",
      "[8,  3000] loss: 72.412\n",
      "[8,  3500] loss: 69.544\n",
      "[8,  4000] loss: 73.521\n",
      "[8,  4500] loss: 74.269\n",
      "[8,  5000] loss: 71.611\n",
      "[8,  5500] loss: 70.848\n",
      "[8,  6000] loss: 70.067\n",
      "[8,  6500] loss: 72.245\n",
      "[8,  7000] loss: 73.355\n",
      "[8,  7500] loss: 70.833\n",
      "[8,  8000] loss: 73.062\n",
      "[8,  8500] loss: 71.240\n",
      "[8,  9000] loss: 69.963\n",
      "[8,  9500] loss: 74.915\n",
      "[8, 10000] loss: 71.834\n",
      "[8, 10500] loss: 73.635\n",
      "Starting epoch 9\n",
      "[9,   500] loss: 65.552\n",
      "[9,  1000] loss: 65.838\n",
      "[9,  1500] loss: 68.179\n",
      "[9,  2000] loss: 66.568\n",
      "[9,  2500] loss: 70.140\n",
      "[9,  3000] loss: 69.136\n",
      "[9,  3500] loss: 66.786\n",
      "[9,  4000] loss: 68.462\n",
      "[9,  4500] loss: 68.309\n",
      "[9,  5000] loss: 65.663\n",
      "[9,  5500] loss: 67.019\n",
      "[9,  6000] loss: 70.861\n",
      "[9,  6500] loss: 67.621\n",
      "[9,  7000] loss: 66.508\n",
      "[9,  7500] loss: 67.262\n",
      "[9,  8000] loss: 66.910\n",
      "[9,  8500] loss: 68.057\n",
      "[9,  9000] loss: 70.457\n",
      "[9,  9500] loss: 67.941\n",
      "[9, 10000] loss: 68.528\n",
      "[9, 10500] loss: 66.811\n",
      "Starting epoch 10\n",
      "[10,   500] loss: 60.626\n",
      "[10,  1000] loss: 64.801\n",
      "[10,  1500] loss: 62.806\n",
      "[10,  2000] loss: 63.523\n",
      "[10,  2500] loss: 63.201\n",
      "[10,  3000] loss: 64.662\n",
      "[10,  3500] loss: 64.157\n",
      "[10,  4000] loss: 62.022\n",
      "[10,  4500] loss: 66.866\n",
      "[10,  5000] loss: 62.535\n",
      "[10,  5500] loss: 64.300\n",
      "[10,  6000] loss: 62.966\n",
      "[10,  6500] loss: 65.051\n",
      "[10,  7000] loss: 59.973\n",
      "[10,  7500] loss: 64.422\n",
      "[10,  8000] loss: 66.591\n",
      "[10,  8500] loss: 64.547\n",
      "[10,  9000] loss: 65.035\n",
      "[10,  9500] loss: 66.672\n",
      "[10, 10000] loss: 67.871\n",
      "[10, 10500] loss: 68.045\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "reload(StepPlacement)\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "conv_model = StepPlacement.ConvStepPlacementModel()\n",
    "conv_model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(conv_model.parameters())\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(\"Starting epoch\", epoch+1)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        labels = data['labels'].cuda()\n",
    "        fft_features = data['fft_features'].cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = conv_model(fft_features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        accuracy = torch.mean(torch.abs(labels - outputs))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         print(loss.item())\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 500 == 499:    # print every 2000 mini-batches\n",
    "#             print('[%d, %5d] loss: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / 500))\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T08:00:34.094367Z",
     "start_time": "2019-02-19T08:00:34.091748Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, auc, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T08:00:34.118966Z",
     "start_time": "2019-02-19T08:00:34.095912Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(SMDataset)\n",
    "# sampler = SMDataset.WeightedRandomSampler(test_ds)\n",
    "test_loader = datautils.DataLoader(\n",
    "    test_ds, num_workers=4, \n",
    "    batch_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T08:00:42.023094Z",
     "start_time": "2019-02-19T08:00:34.120658Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXX\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "tensor([1., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "tensor(0.2700, device='cuda:0')\n",
      "XXXXXXXXXXX\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "tensor(0.2630, device='cuda:0')\n",
      "XXXXXXXXXXX\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "tensor(0.2659, device='cuda:0')\n",
      "XXXXXXXXXXX\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 1., 0.]], device='cuda:0')\n",
      "tensor([1., 0., 0.,  ..., 1., 1., 0.], device='cuda:0')\n",
      "tensor(0.2620, device='cuda:0')\n",
      "XXXXXXXXXXX\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.]], device='cuda:0')\n",
      "tensor([0., 0., 1.,  ..., 0., 1., 0.], device='cuda:0')\n",
      "tensor(0.2675, device='cuda:0')\n",
      "XXXXXXXXXXX\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 1.], device='cuda:0')\n",
      "tensor(0.2629, device='cuda:0')\n",
      "XXXXXXXXXXX\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]], device='cuda:0')\n",
      "tensor([1., 0., 0.,  ..., 1., 0., 0.], device='cuda:0')\n",
      "tensor(0.2687, device='cuda:0')\n",
      "XXXXXXXXXXX\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 1., 0.]], device='cuda:0')\n",
      "tensor([0., 0., 0.,  ..., 0., 1., 0.], device='cuda:0')\n",
      "tensor(0.2677, device='cuda:0')\n",
      "XXXXXXXXXXX\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "tensor(0.2690, device='cuda:0')\n",
      "XXXXXXXXXXX\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "tensor([0., 1., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "tensor(0.2661, device='cuda:0')\n",
      "XXXXXXXXXXX\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 1., 0.]], device='cuda:0')\n",
      "tensor([0., 1., 0.,  ..., 0., 1., 0.], device='cuda:0')\n",
      "tensor(0.2607, device='cuda:0')\n",
      "XXXXXXXXXXX\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "tensor([0., 0., 1.,  ..., 0., 0., 1.], device='cuda:0')\n",
      "tensor(0.2673, device='cuda:0')\n",
      "Accuracy: 0.7571271806955338\n",
      "ROC: 0.8436559074442894\n",
      "AUC PR: 0.4754787465980061\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    accs = []\n",
    "    for d in test_loader:\n",
    "        labels = d['labels'].cuda()\n",
    "        fft_features = d['fft_features'].cuda()\n",
    "        \n",
    "        outputs = conv_model(fft_features)\n",
    "#         preds = torch.round(outputs)\n",
    "        preds = (outputs > 0.06).float()\n",
    "        \n",
    "        accuracy = torch.mean(torch.abs(labels - preds))\n",
    "        print(\"XXXXXXXXXXX\")\n",
    "        print(torch.stack([labels.reshape(-1), preds.reshape(-1)]))\n",
    "        print(torch.abs(labels.reshape(-1) - preds.reshape(-1)))\n",
    "        print(torch.mean(preds))\n",
    "        accs.append(accuracy)\n",
    "        \n",
    "    print(\"Accuracy:\", 1 - np.mean(list(map(lambda acc: acc.cpu().numpy(), accs))))\n",
    "    \n",
    "    roc = roc_auc_score(\n",
    "        labels.cpu().numpy().reshape(-1), \n",
    "        outputs.cpu().numpy().reshape(-1))\n",
    "    \n",
    "    print(\"ROC:\", roc)\n",
    "    \n",
    "    precision, recall, thresh = precision_recall_curve(\n",
    "        labels.cpu().numpy().reshape(-1), \n",
    "        outputs.cpu().numpy().reshape(-1))\n",
    "    \n",
    "    prauc = auc(recall, precision)\n",
    "    print(\"AUC PR:\", prauc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T08:00:42.159436Z",
     "start_time": "2019-02-19T08:00:42.025252Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe5e41837f0>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIimkcBcRSSGFu4hICv0ndGB4ndKA98IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe5e41e9198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
